{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fvXaBlVEqVBT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-889dd20e7eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyemd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process\n",
    "from pyemd import emd\n",
    "# from gensim.similarities import WmdSimilarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohy9Un3DszFW",
    "outputId": "cd0ee831-a986-4044-d69a-25e1310b939d"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WSuP1I3lxEs5"
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-YClr0NsxJns"
   },
   "outputs": [],
   "source": [
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qx51Q80Cr8tL"
   },
   "outputs": [],
   "source": [
    "def sentence_similarity(model_key, student_key):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    model_answer_tokenized = word_tokenize(model_key)\n",
    "    model_pos_tagged = pos_tag(model_answer_tokenized)\n",
    "    model_tagged = [(lemmatizer.lemmatize(word[0].lower(),penn_to_wn(word[1])),word[1]) for word in model_pos_tagged]\n",
    "#     model_lemmatized = [lemmatizer.lemmatize(word[0].lower(),penn_to_wn(word[1])) for word in model_pos_tagged]\n",
    "#     model_tagged = pos_tag(model_lemmatized)\n",
    "#     print(model_tagged)\n",
    "#     model_tagged = pos_tag(model_answer_words)\n",
    "\n",
    "    student_answer_tokenized = word_tokenize(student_key)\n",
    "    student_pos_tagged = pos_tag(student_answer_tokenized)\n",
    "    student_tagged = [(lemmatizer.lemmatize(word[0].lower(),penn_to_wn(word[1])),word[1]) for word in student_pos_tagged]\n",
    "#     student_lemmatized = [lemmatizer.lemmatize(word[0].lower(),penn_to_wn(word[1])) for word in student_pos_tagged]\n",
    "#     student_tagged = pos_tag(student_lemmatized)\n",
    "#     print(student_tagged)\n",
    "    \n",
    "#     model_tagged = pos_tag(lemmatizer.lemmatize(word_tokenize(model_key)))\n",
    "#     student_tagged = pos_tag(lemmatizer.lemmatize(word_tokenize(student_key)))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    model_syn = [tagged_to_synset(*tagged_word) for tagged_word in model_tagged]\n",
    "    student_syn = [tagged_to_synset(*tagged_word) for tagged_word in student_tagged]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    model_syn_filtered = [ss for ss in model_syn if ss]\n",
    "    student_syn_filtered = [ss for ss in student_syn if ss]\n",
    "    print(model_syn_filtered, student_syn_filtered)\n",
    " \n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    for word in model_syn_filtered:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "#         scores = [word.path_similarity(ss) for ss in student_syn_filtered]\n",
    "        scores = [word.wup_similarity(ss) for ss in student_syn_filtered]\n",
    "#         print(scores)\n",
    "        scores = [s for s in scores if s]\n",
    "        print(scores)\n",
    "        if len(scores)!=0:\n",
    "            best_score = max(scores)\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    if count!=0:\n",
    "        score /= count\n",
    "        return score\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAIHomE4xUeI",
    "outputId": "83db2fa0-02ce-4905-975f-1e77c5ca39e6"
   },
   "outputs": [],
   "source": [
    "answers1 = []\n",
    "f = open('answers1.txt')\n",
    "lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if line!='\\n':\n",
    "        answers1 += [line.replace('\\n', '')]\n",
    "# print(answers1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s00iiLeyxmV7",
    "outputId": "af9d2217-1dd2-44d7-f30f-62ec63c6b514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type before reconstruction :  <class 'str'>\n",
      "Data type after reconstruction :  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('model_answers.txt') as f: \n",
    "    data = f.read() \n",
    "  \n",
    "print(\"Data type before reconstruction : \", type(data)) \n",
    "      \n",
    "# reconstructing the data as a dictionary \n",
    "model_answer_list = json.loads(data) \n",
    "  \n",
    "print(\"Data type after reconstruction : \", type(model_answer_list)) \n",
    "# print(model_answer_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nQTwScilzEL8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Enhance the responsiveness to the users.': 0.5, 'Resource sharing within the process.': 0.5, 'Economical': 0.5, 'Completely utilize the multiprocessing architecture.': 0.5}\n",
      "Responsiveness, Resource Sharing, Economy, Scalability\n"
     ]
    }
   ],
   "source": [
    "model_answer = model_answer_list[-2]\n",
    "student_answer =  answers1[-2]\n",
    "\n",
    "print(model_answer)\n",
    "print(student_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0thycYYgzORq",
    "outputId": "98908c22-b057-4dfc-d204-09f184ca9b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resource sharing', 'scalability', 'responsiveness', 'economy']\n",
      "['users', 'responsiveness', 'enhance']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-34510a9f78c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence_keyword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_keywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstudent_keyword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstudent_keywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_keyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_keyword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudent_keyword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentence_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_keyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_keyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\sarika\\python\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\sarika\\python\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1903\u001b[0m         \u001b[1;31m# 0. Check the exception lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1905\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1906\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mform\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "r = Rake()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "r.extract_keywords_from_text(student_answer)\n",
    "student_keywords = r.get_ranked_phrases()\n",
    "print(student_keywords)\n",
    "for sentence in model_answer.keys():\n",
    "    r.extract_keywords_from_text(sentence)\n",
    "    sentence_keywords = r.get_ranked_phrases()\n",
    "    print(sentence_keywords)\n",
    "    for sentence_keyword in sentence_keywords:\n",
    "        for student_keyword in student_keywords:\n",
    "            print(lemmatizer.lemmatize([x for x in word_tokenize(sentence_keyword)]))\n",
    "            print(sentence_keyword,\",\",student_keyword,\",\",sentence_similarity(sentence_keyword, student_keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "_KWwYGxn20Jj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "[Synset('new.a.01'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "new process , new , 1.0\n",
      "1.0\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('run.n.01'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "running process , running , 0.7777777777777778\n",
      "0.7777777777777778\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('delay.n.01'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "waiting process , waiting , 0.6538461538461539\n",
      "0.6538461538461539\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('ready.n.01'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "ready process , ready , 0.6538461538461539\n",
      "0.6538461538461539\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('delay.n.01')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('end.v.02')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('run.n.01')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('ready.n.01')]\n",
      "[Synset('end.v.02'), Synset('procedure.n.01')] [Synset('new.a.01')]\n",
      "terminated process , terminate , 1.0\n",
      "1.0\n",
      "0.8170940170940171\n"
     ]
    }
   ],
   "source": [
    "# r = Rake()\n",
    "# r.extract_keywords_from_text(student_answer)\n",
    "# student_keywords = r.get_ranked_phrases()\n",
    "# grand_total = 0\n",
    "# for sentence in model_answer.keys():\n",
    "#     r.extract_keywords_from_text(sentence)\n",
    "#     sentence_keywords = r.get_ranked_phrases()\n",
    "#     total = 0\n",
    "#     for sentence_keyword in sentence_keywords:\n",
    "#         best =  None\n",
    "#         max_score = 0\n",
    "#         for student_keyword in student_keywords:\n",
    "#             if sentence_similarity(sentence_keyword, student_keyword)>max_score:\n",
    "#                 max_score = sentence_similarity(sentence_keyword, student_keyword)\n",
    "#                 best = student_keyword\n",
    "#         total+=max_score\n",
    "#         print(sentence_keyword,\",\",best,\",\",max_score)\n",
    "#     print(total/len(sentence_keywords))\n",
    "#     grand_total += total/len(sentence_keywords)\n",
    "\n",
    "# print(grand_total/len(model_answer.keys()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
